{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66a4fd52",
   "metadata": {},
   "source": [
    "# Fixed Income Asset Pricing Solutions\n",
    "\n",
    "**Branch:** jules\n",
    "**Author:** Jules (AI Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb382db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import array, mean, std, var, sqrt, arange, zeros, ones, nonzero, diag, exp, log, cumsum, tile, transpose, concatenate, diff, interp, stack, vstack, hstack\n",
    "from numpy.linalg import inv, eigh\n",
    "from scipy.stats import norm\n",
    "from scipy.interpolate import interp1d, PchipInterpolator\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Setting up plot styles\n",
    "pio.templates.default = \"plotly_white\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcac03f4",
   "metadata": {},
   "source": [
    "## PSET 1: Time Series Analysis of T-Bill Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b57c493",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Solving PSET 1...\")\n",
    "\n",
    "file_path = 'Assignments/PSET 1/DTB3_2024.xls'\n",
    "\n",
    "# =================== Question 1 ===========================\n",
    "print(\"Question 1: Determining time series of BEY and providing its plot\")\n",
    "\n",
    "try:\n",
    "    # DTB3 sheet has header in row 10 (0-indexed) -> usually means row 11 is data?\n",
    "    # Guide says skiprows=10, names=['DATE','DTB3']\n",
    "    data_tbill = pd.read_excel(file_path, sheet_name='DTB3', skiprows=10, header=None)\n",
    "    data_tbill.columns = ['DATE', 'DTB3']\n",
    "except Exception as e:\n",
    "    print(f\"Error reading excel: {e}\")\n",
    "    return\n",
    "\n",
    "data_tbill['DTB3'] = pd.to_numeric(data_tbill['DTB3'], errors='coerce')\n",
    "data_tbill = data_tbill.dropna()\n",
    "\n",
    "rates = data_tbill['DTB3'].values / 100\n",
    "dates = data_tbill['DATE'].values\n",
    "\n",
    "N1 = 365\n",
    "N2 = 360\n",
    "N3 = 91\n",
    "\n",
    "BEY = N1 * rates / (N2 - rates * N3)\n",
    "\n",
    "fig1 = go.Figure()\n",
    "fig1.add_trace(go.Scatter(x=dates, y=rates, mode='lines', name='Quoted discounts'))\n",
    "fig1.add_trace(go.Scatter(x=dates, y=BEY, mode='lines', name='BEY'))\n",
    "fig1.update_layout(title='3-month T-bill rates', xaxis_title='Date', yaxis_title='Rate', legend_title='Type')\n",
    "\n",
    "# =================== Question 2 ===========================\n",
    "print(\"Question 2: Estimating AR(1) process\")\n",
    "\n",
    "m = len(BEY) - 1\n",
    "Y = BEY[1:]\n",
    "X = BEY[0:-1]\n",
    "\n",
    "cov_matrix = np.cov(X, Y)\n",
    "s = cov_matrix[0, 1]\n",
    "var_x = np.var(X, ddof=1)\n",
    "\n",
    "beta_hat = s / var_x\n",
    "alpha_hat = mean(Y) - beta_hat * mean(X)\n",
    "\n",
    "eps = Y - alpha_hat - beta_hat * X\n",
    "sig = np.sqrt(np.var(eps, ddof=1)) \n",
    "\n",
    "print('Regression coefficients for rates:')\n",
    "print(f'beta_hat = {beta_hat}')\n",
    "print(f'alpha_hat = {alpha_hat}')\n",
    "print(f'sig = {sig}')\n",
    "\n",
    "# =================== Question 3 ===========================\n",
    "print(\"Question 3: Forecast\")\n",
    "\n",
    "n = 5 # years\n",
    "days = n * 252\n",
    "rate_forecast = zeros(days + 1)\n",
    "rate_forecast[0] = BEY[-1]\n",
    "\n",
    "for i in range(days):\n",
    "    rate_forecast[i+1] = alpha_hat + beta_hat * rate_forecast[i]\n",
    "    \n",
    "LR_mean = alpha_hat / (1 - beta_hat)\n",
    "\n",
    "t_future = arange(0, n + 1/252, 1/252)\n",
    "if len(t_future) > len(rate_forecast):\n",
    "    t_future = t_future[:len(rate_forecast)]\n",
    "elif len(rate_forecast) > len(t_future):\n",
    "    rate_forecast = rate_forecast[:len(t_future)]\n",
    "\n",
    "fig2 = go.Figure()\n",
    "fig2.add_trace(go.Scatter(x=t_future, y=rate_forecast, mode='lines', name='Time Series Forecast'))\n",
    "fig2.add_trace(go.Scatter(x=t_future, y=[LR_mean]*len(t_future), mode='lines', name='Long-term interest rate', line=dict(dash='dash')))\n",
    "fig2.update_layout(title='Forecast of 3-month T-bill rates', xaxis_title='Forecasting horizon (years)', yaxis_title='Rate')\n",
    "\n",
    "# =================== Question 4 ===========================\n",
    "print(\"Question 4: Yield curve and forward rates\")\n",
    "\n",
    "# Reading with header=0 to pick up \"Maturity in Years\" and \"Bond Price (Face = $1)\"\n",
    "strips_data = pd.read_excel(file_path, sheet_name='Strip Prices', header=0)\n",
    "strips_data.columns = ['Mat', 'Price']\n",
    "\n",
    "# Force convert to numeric\n",
    "strips_data['Mat'] = pd.to_numeric(strips_data['Mat'], errors='coerce')\n",
    "strips_data['Price'] = pd.to_numeric(strips_data['Price'], errors='coerce')\n",
    "\n",
    "strips_data = strips_data.dropna()\n",
    "\n",
    "Mat = strips_data['Mat'].values\n",
    "Zfun = strips_data['Price'].values\n",
    "\n",
    "mask = Mat < (n + 0.25)\n",
    "Mat = Mat[mask]\n",
    "Zfun = Zfun[mask]\n",
    "\n",
    "# Yield calculation\n",
    "# Zfun is Price per $1 face value.\n",
    "# Using semi-annual compounding BEY: y = 2 * ( (1/P)^(1/2T) - 1 )\n",
    "yield1 = 2 * ((1.0 / Zfun)**(1 / (2 * Mat)) - 1)\n",
    "\n",
    "sorted_indices = np.argsort(Mat)\n",
    "Mat_sorted = Mat[sorted_indices]\n",
    "Zfun_sorted = Zfun[sorted_indices]\n",
    "\n",
    "Z_interp = interp1d(Mat_sorted, Zfun_sorted, kind='linear', fill_value=\"extrapolate\")\n",
    "\n",
    "t_points = t_future\n",
    "delta = 0.25\n",
    "\n",
    "Z_t = Z_interp(t_points)\n",
    "Z_t_delta = Z_interp(t_points + delta)\n",
    "\n",
    "# Forward rate for [t, t+delta]\n",
    "fwds = 2 * ((Z_t / Z_t_delta)**(1 / (2 * delta)) - 1)\n",
    "\n",
    "fig3 = go.Figure()\n",
    "fig3.add_trace(go.Scatter(x=Mat, y=Zfun, mode='lines', name='Z function'))\n",
    "fig3.update_layout(title='Discount function', xaxis_title='Maturity (years)', yaxis_title='Price')\n",
    "\n",
    "Z_Mat = Zfun\n",
    "Z_Mat_delta = Z_interp(Mat + delta)\n",
    "fwds_Mat = 2 * ((Z_Mat / Z_Mat_delta)**(1 / (2 * delta)) - 1)\n",
    "\n",
    "fig4 = go.Figure()\n",
    "fig4.add_trace(go.Scatter(x=Mat, y=yield1, mode='lines', name='Yield'))\n",
    "fig4.add_trace(go.Scatter(x=Mat, y=fwds_Mat, mode='lines', name='Forward', line=dict(dash='dashdot')))\n",
    "fig4.update_layout(title='Yields and Forwards', xaxis_title='Maturity', yaxis_title='Spot rate')\n",
    "\n",
    "fig5 = go.Figure()\n",
    "fig5.add_trace(go.Scatter(x=t_points, y=fwds, mode='lines', name='Forward', line=dict(dash='dashdot')))\n",
    "fig5.add_trace(go.Scatter(x=t_points, y=rate_forecast, mode='lines', name='Time Series Forecast'))\n",
    "fig5.update_layout(title='Two forecasts of future interest rates', xaxis_title='Forecasting horizon (years)', yaxis_title='Spot rate')\n",
    "\n",
    "return {\n",
    "    \"beta_hat\": beta_hat,\n",
    "    \"alpha_hat\": alpha_hat,\n",
    "    \"sig\": sig,\n",
    "    \"fig1\": fig1,\n",
    "    \"fig2\": fig2,\n",
    "    \"fig3\": fig3,\n",
    "    \"fig4\": fig4,\n",
    "    \"fig5\": fig5\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4b4e3b",
   "metadata": {},
   "source": [
    "The code above solves PSET 1. The results and plots are generated within the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ca5831",
   "metadata": {},
   "source": [
    "## PSET 2: Term Structure Bootstrapping and LIF Pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7368e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Solving PSET 2...\")\n",
    "\n",
    "file_path = 'Assignments/PSET 2/HW2_Data.xls'\n",
    "\n",
    "# =================== Question 1: Bootstrapping the term structure ===========================\n",
    "print(\"Question 1: Bootstrapping the term structure\")\n",
    "\n",
    "# Extract data from AllBondQuotes\n",
    "try:\n",
    "    raw_df = pd.read_excel(file_path, sheet_name='AllBondQuotes_20090217', header=9)\n",
    "    # Columns: crspid, qdate, name, matdt, type, couprt, bid, ask, Time to Maturity\n",
    "    # Note: 'type' column might be named 'type' or similar.\n",
    "    # Let's clean column names\n",
    "    raw_df.columns = [c.strip().lower() for c in raw_df.columns]\n",
    "    # Map known columns\n",
    "    # 'time to maturity' -> 'ttm'\n",
    "    # 'couprt' -> 'coupon'\n",
    "    \n",
    "    # Check if 'time to maturity' exists\n",
    "    if 'time to maturity' in raw_df.columns:\n",
    "        raw_df.rename(columns={'time to maturity': 'ttm'}, inplace=True)\n",
    "    \n",
    "    # Filter non-callable (type 1 or 2)\n",
    "    # Note: 'type' might be float.\n",
    "    df = raw_df[raw_df['type'].isin([1, 2])].copy()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error reading bond data: {e}\")\n",
    "    return\n",
    "\n",
    "# Define targets\n",
    "targets = arange(0.5, 13.0, 0.5)\n",
    "\n",
    "old_bonds = []\n",
    "new_bonds = []\n",
    "\n",
    "valid_targets = []\n",
    "\n",
    "for T in targets:\n",
    "    # Find bonds with TTM close to T\n",
    "    # Tolerance 0.06 to capture roughly +/- 20 days\n",
    "    mask = (df['ttm'] - T).abs() < 0.08\n",
    "    candidates = df[mask].copy()\n",
    "    \n",
    "    if candidates.empty:\n",
    "        print(f\"No bond found for maturity {T}\")\n",
    "        break\n",
    "        \n",
    "    valid_targets.append(T)\n",
    "    \n",
    "    # Sort by coupon\n",
    "    candidates = candidates.sort_values(by='couprt')\n",
    "    \n",
    "    # New Bond = Lowest Coupon (often On-the-run / closer to par in low yield env?)\n",
    "    # Actually, let's pick the one with TTM closest to T for \"New\"\n",
    "    # and another one for \"Old\".\n",
    "    \n",
    "    # Refined strategy:\n",
    "    # Sort by |TTM - T| to get closest maturity\n",
    "    candidates['dist'] = (candidates['ttm'] - T).abs()\n",
    "    candidates_sorted = candidates.sort_values(by='dist')\n",
    "    \n",
    "    best_match = candidates_sorted.iloc[0]\n",
    "    new_bonds.append([best_match['couprt'], best_match['bid'], best_match['ask'], best_match['ttm']])\n",
    "    \n",
    "    # For Old bond, try to pick one with significantly different coupon or just the second best match?\n",
    "    # If we pick simply based on TTM, we might pick the same bond.\n",
    "    # Let's try to pick a high coupon bond for \"Old\" and low coupon for \"New\"?\n",
    "    # Or follow the logic that Old = Off-the-run.\n",
    "    # Let's select:\n",
    "    # New: Bond with TTM closest to T.\n",
    "    # Old: Bond with TTM closest to T but not the same CRSPID?\n",
    "    # Or just Bond with Highest Coupon?\n",
    "    \n",
    "    # Let's go with: New = Lowest Coupon, Old = Highest Coupon.\n",
    "    candidates_by_c = candidates.sort_values(by='couprt')\n",
    "    low_c = candidates_by_c.iloc[0]\n",
    "    high_c = candidates_by_c.iloc[-1]\n",
    "    \n",
    "    # If they are same, then we don't have distinct old/new.\n",
    "    # We'll use the same.\n",
    "    new_bonds_list = [low_c['couprt'], low_c['bid'], low_c['ask'], low_c['ttm']]\n",
    "    old_bonds_list = [high_c['couprt'], high_c['bid'], high_c['ask'], high_c['ttm']]\n",
    "    \n",
    "    # The lists in \"new_bonds\" variable (which accumulates rows)\n",
    "    # Note: I'm overwriting the logic.\n",
    "    # Let's reset.\n",
    "    pass\n",
    "\n",
    "# Re-loop to fill lists\n",
    "old_bonds = []\n",
    "new_bonds = []\n",
    "final_targets = []\n",
    "\n",
    "for T in targets:\n",
    "    mask = (df['ttm'] - T).abs() < 0.08\n",
    "    candidates = df[mask].copy()\n",
    "    \n",
    "    if candidates.empty:\n",
    "        break\n",
    "        \n",
    "    final_targets.append(T)\n",
    "    \n",
    "    # Sort by coupon\n",
    "    candidates_by_c = candidates.sort_values(by='couprt')\n",
    "    \n",
    "    # New = Lowest Coupon (Set 1 in loop i=1)\n",
    "    # Old = Highest Coupon (Set 0 in loop i=0)\n",
    "    \n",
    "    low_c = candidates_by_c.iloc[0]\n",
    "    high_c = candidates_by_c.iloc[-1]\n",
    "    \n",
    "    new_bonds.append([low_c['couprt'], low_c['bid'], low_c['ask'], low_c['ttm']])\n",
    "    old_bonds.append([high_c['couprt'], high_c['bid'], high_c['ask'], high_c['ttm']])\n",
    "    \n",
    "old_bonds = array(old_bonds)\n",
    "new_bonds = array(new_bonds)\n",
    "\n",
    "# Bootstrapping Loop\n",
    "results = {}\n",
    "\n",
    "fig1 = go.Figure()\n",
    "fig2 = go.Figure()\n",
    "\n",
    "z_curves = []\n",
    "y_curves = []\n",
    "\n",
    "datasets = [old_bonds, new_bonds]\n",
    "labels = ['Use Old Bonds', 'Use New Bonds']\n",
    "\n",
    "for i in range(2):\n",
    "    bond_data = datasets[i]\n",
    "    \n",
    "    coupon = bond_data[:, 0]\n",
    "    bid = bond_data[:, 1]\n",
    "    ask = bond_data[:, 2]\n",
    "    maturity = bond_data[:, 3]\n",
    "    \n",
    "    Nmat = len(maturity)\n",
    "    freq = 2\n",
    "    price = (bid + ask) / 2\n",
    "    maturity_rounded = np.round(maturity, 1) # Should match 0.5, 1.0 ...\n",
    "    \n",
    "    # Bootstrap Matrix\n",
    "    CF = zeros((Nmat, Nmat))\n",
    "    for ii in range(Nmat):\n",
    "        # Coupon payments\n",
    "        # Bond ii matures at T_ii = 0.5 * (ii+1) roughly\n",
    "        # It pays coupons at T_0, T_1 ... T_ii\n",
    "        # So CF[ii, 0:ii+1] = coupon[ii]/freq\n",
    "        CF[ii, 0:ii+1] = coupon[ii] / freq\n",
    "        \n",
    "    CF = CF + 100 * np.eye(Nmat)\n",
    "    \n",
    "    try:\n",
    "        Z = inv(CF) @ price\n",
    "    except:\n",
    "        # Fallback if singular (e.g. duplicate maturities?)\n",
    "        # Add small epsilon to diagonal?\n",
    "        Z = inv(CF + 1e-9*np.eye(Nmat)) @ price\n",
    "    \n",
    "    # Semi-annual yield\n",
    "    yield_semi = 2 * ((1/Z)**(1/(2*maturity)) - 1) # Use actual maturity T\n",
    "    \n",
    "    z_curves.append(Z)\n",
    "    y_curves.append(yield_semi)\n",
    "    \n",
    "    fig1.add_trace(go.Scatter(x=maturity, y=Z, mode='lines', name=labels[i]))\n",
    "    fig2.add_trace(go.Scatter(x=maturity, y=yield_semi, mode='lines', name=labels[i]))\n",
    "    \n",
    "fig1.update_layout(title='Bootstrapped Discount', xaxis_title='Maturity', yaxis_title='Discount Factor')\n",
    "fig2.update_layout(title='Bootstrapped Term Structure', xaxis_title='Maturity', yaxis_title='Yield')\n",
    "\n",
    "# Use \"New Bonds\" (index 1) for subsequent questions\n",
    "Z = z_curves[1]\n",
    "yield1 = y_curves[1]\n",
    "# Re-define maturity vector based on the New Bonds set\n",
    "maturity = new_bonds[:, 3]\n",
    "\n",
    "# =================== Question 2: Pricing Leveraged Inverse Floater ===========================\n",
    "print(\"Question 2: Pricing Leveraged Inverse Floater\")\n",
    "\n",
    "coup_fix = 10\n",
    "T = 5\n",
    "freq = 2\n",
    "\n",
    "# Ensure we have enough data\n",
    "# We need maturity up to 5.0. \n",
    "# Indices: 0->0.5, ..., 9->5.0\n",
    "if len(Z) < 10:\n",
    "    print(\"Not enough data for 5 year pricing\")\n",
    "    return\n",
    "    \n",
    "CF_fixed = (coup_fix / freq) * ones(T * freq)\n",
    "CF_fixed[-1] += 100\n",
    "\n",
    "P_Fixed = Z[0:T*freq] @ CF_fixed\n",
    "P_Float = 100\n",
    "P_zero = 100 * Z[T*freq - 1]\n",
    "\n",
    "P_LIF = P_Fixed - 2 * P_Float + 2 * P_zero\n",
    "print(f'Price of Inverse Floater: {P_LIF:.4f}')\n",
    "\n",
    "# =================== Question 3: Duration and Convexity ===========================\n",
    "print(\"Question 3: Duration and Convexity analysis\")\n",
    "\n",
    "# Duration weights\n",
    "stripweights = (Z[0:freq*T] / P_Fixed) * (coup_fix / 2)\n",
    "stripweights[-1] += (Z[freq*T-1] * 100) / P_Fixed\n",
    "\n",
    "D_Fixed = stripweights @ maturity[0:freq*T]\n",
    "D_Float = 0.5 # As discussed\n",
    "D_Zero = maturity[freq*T - 1]\n",
    "\n",
    "D_LIF = (D_Fixed * P_Fixed - 2 * D_Float * P_Float + 2 * D_Zero * P_zero) / P_LIF\n",
    "\n",
    "# Fixed 5-year bond duration (from New set)\n",
    "# The bond at index 9 corresponds to 5.0 years.\n",
    "C_5yr = new_bonds[9, 0]\n",
    "P_5yr = (new_bonds[9, 1] + new_bonds[9, 2]) / 2\n",
    "mat_5yr = new_bonds[9, 3]\n",
    "\n",
    "# Recalculate duration for this specific bond using bootstrapped Z\n",
    "# CF for this bond\n",
    "CF_5yr = (C_5yr / freq) * ones(T * freq)\n",
    "CF_5yr[-1] += 100\n",
    "\n",
    "# Weights\n",
    "w_5yr = (Z[0:freq*T] * CF_5yr) / P_5yr # Element-wise mult\n",
    "# Wait, Price should equal sum(Z * CF).\n",
    "# Does P_5yr match sum(Z * CF_5yr)? It should if Z solves CF*Z=P.\n",
    "# Check discrepancy\n",
    "P_model = Z[0:freq*T] @ CF_5yr\n",
    "# Use P_model for consistency in duration calc\n",
    "\n",
    "w_5yr = (Z[0:freq*T] * CF_5yr) / P_model\n",
    "D_fixed5 = w_5yr @ maturity[0:freq*T]\n",
    "\n",
    "print(f'Duration of LIF: {D_LIF:.2f}')\n",
    "print(f'Duration of Fixed 5yr: {D_fixed5:.2f}')\n",
    "\n",
    "# Plot sensitivity\n",
    "yshift = arange(-0.005, 0.05 + 0.0005, 0.0005)\n",
    "Nyshift = len(yshift)\n",
    "Plif_shift = zeros(Nyshift)\n",
    "Pfixed5_shift = zeros(Nyshift)\n",
    "\n",
    "# Continuous yield for shifting\n",
    "yield_cont = -np.log(Z) / maturity\n",
    "\n",
    "for ii in range(Nyshift):\n",
    "    y_shifted_cont = yield_cont + yshift[ii]\n",
    "    Zshift = exp(-y_shifted_cont * maturity)\n",
    "    \n",
    "    Pfixed_shift = np.sum(Zshift[0:freq*T]) * coup_fix/2 + Zshift[freq*T-1]*100\n",
    "    Plif_shift[ii] = Pfixed_shift - 2*P_Float + 2*Zshift[freq*T-1]*100\n",
    "    \n",
    "    Pfixed5_shift[ii] = np.sum(Zshift[0:freq*T]) * C_5yr/2 + Zshift[freq*T-1]*100\n",
    "    \n",
    "fig3 = go.Figure()\n",
    "fig3.add_trace(go.Scatter(x=yshift, y=Plif_shift, mode='lines', name='Leveraged Inverse Floater'))\n",
    "fig3.add_trace(go.Scatter(x=yshift, y=Pfixed5_shift, mode='lines', name='Fixed Rate 5-yr Bond', line=dict(dash='dashdot')))\n",
    "fig3.update_layout(title='Price Sensitivity', xaxis_title='Size of parallel shift', yaxis_title='Price')\n",
    "\n",
    "# Convexity\n",
    "C_Fixed_Conv = stripweights @ (maturity[0:freq*T]**2)\n",
    "C_Float_Conv = (1/freq)**2\n",
    "C_Zero_Conv = maturity[freq*T-1]**2\n",
    "\n",
    "C_LIF = (C_Fixed_Conv * P_Fixed - 2 * C_Float_Conv * P_Float + 2 * C_Zero_Conv * P_zero) / P_LIF\n",
    "\n",
    "C_fixed5 = w_5yr @ (maturity[0:freq*T]**2)\n",
    "\n",
    "print(f'Convexity of LIF: {C_LIF:.2f}')\n",
    "print(f'Convexity of 5-yr fixed: {C_fixed5:.2f}')\n",
    "\n",
    "# =================== Question 4: Value at Risk ===========================\n",
    "print(\"Question 4: Value at Risk calculation\")\n",
    "\n",
    "try:\n",
    "    # DTB6 sheet\n",
    "    d6_data = pd.read_excel(file_path, sheet_name='DTB6', skiprows=5, usecols=[1], header=None)\n",
    "    d6_data.columns = ['d6']\n",
    "    d6_data['d6'] = pd.to_numeric(d6_data['d6'], errors='coerce')\n",
    "    d6 = d6_data['d6'].dropna().values\n",
    "    \n",
    "    n_days = 182\n",
    "    P6 = 1 - (d6 / 100) * (n_days / 360) \n",
    "    \n",
    "    r6 = -np.log(P6) * 2\n",
    "    dr6 = np.diff(r6)\n",
    "    \n",
    "    mu6 = np.mean(dr6)\n",
    "    sig6 = np.std(dr6, ddof=1)\n",
    "    \n",
    "    mu_LIF = -D_LIF * P_LIF * mu6\n",
    "    sig_LIF = abs(-D_LIF * P_LIF * sig6)\n",
    "    \n",
    "    VaR95 = 1.645 * sig_LIF - mu_LIF\n",
    "    VaR99 = 2.33 * sig_LIF - mu_LIF\n",
    "    \n",
    "    dP_LIF = -D_LIF * P_LIF * dr6\n",
    "    VaR95_H = -np.percentile(dP_LIF, 5)\n",
    "    VaR99_H = -np.percentile(dP_LIF, 1)\n",
    "    \n",
    "    print(f'95% Normal VaR LIF: {VaR95:.2f}')\n",
    "    print(f'95% Hist VaR LIF: {VaR95_H:.2f}')\n",
    "    print(f'99% Normal VaR LIF: {VaR99:.2f}')\n",
    "    print(f'99% Hist VaR LIF: {VaR99_H:.2f}')\n",
    "    \n",
    "    fig4 = go.Figure()\n",
    "    fig4.add_trace(go.Histogram(x=dP_LIF, histnorm='probability density', name='Historical Distribution', marker_color='cyan', opacity=0.75))\n",
    "    fig4.update_layout(title='VaR and Historical Distribution for LIF', xaxis_title='Change in Price', yaxis_title='Density')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in VaR calc: {e}\")\n",
    "    VaR95, VaR99, VaR95_H, VaR99_H = 0,0,0,0\n",
    "    fig4 = go.Figure()\n",
    "\n",
    "return {\n",
    "    \"P_LIF\": P_LIF,\n",
    "    \"D_LIF\": D_LIF,\n",
    "    \"C_LIF\": C_LIF,\n",
    "    \"VaR95\": VaR95,\n",
    "    \"fig1\": fig1,\n",
    "    \"fig2\": fig2,\n",
    "    \"fig3\": fig3,\n",
    "    \"fig4\": fig4\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0518003a",
   "metadata": {},
   "source": [
    "## PSET 3: PCA and Predictability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74287a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Solving PSET 3...\")\n",
    "\n",
    "file_path = 'Assignments/PSET 3/FBYields_2024_v2.xlsx'\n",
    "\n",
    "# =================== PART 1: Principal Component Analysis ===========================\n",
    "print(\"Part 1: PCA\")\n",
    "\n",
    "# Read data\n",
    "# Guide: sheet_name='FBYields', header=None, skiprows=5, usecols='A,C:I'\n",
    "# Columns: A (Date), C to I (Yields for different maturities)\n",
    "# Check maturities. Often 1/12, 3/12, 1, 2, 3, 4, 5\n",
    "\n",
    "try:\n",
    "    data = pd.read_excel(file_path, sheet_name='FBYields', header=None, skiprows=5, usecols=\"A,C:I\")\n",
    "    # Column A is index 0. C:I are indices 2 to 8?\n",
    "    # Actually pandas reads A as 0. C:I implies B is skipped.\n",
    "    # Let's check shape and head.\n",
    "except Exception as e:\n",
    "    print(f\"Error reading FBYields: {e}\")\n",
    "    return\n",
    "\n",
    "data = data.values\n",
    "\n",
    "# Find dates\n",
    "date1 = 20090331\n",
    "date2 = 20090430\n",
    "\n",
    "# Column 0 is date\n",
    "# Ensure dates are integers or matching format\n",
    "dates = data[:, 0]\n",
    "\n",
    "I_1 = dates == date1\n",
    "I_2 = dates == date2\n",
    "\n",
    "if not np.any(I_1) or not np.any(I_2):\n",
    "    print(f\"Dates {date1} or {date2} not found in data\")\n",
    "    # Let's print some dates\n",
    "    print(\"First few dates:\", dates[:5])\n",
    "    return\n",
    "\n",
    "# Term structure on two dates\n",
    "# Maturities: 1/12, 3/12, 1, 2, 3, 4, 5\n",
    "Mat = array([1/12, 3/12, 1, 2, 3, 4, 5])\n",
    "\n",
    "# Interpolation Points: 0.5 to 5.0 step 0.5\n",
    "MatInt = arange(0.5, 5.5, 0.5)\n",
    "\n",
    "yields1 = data[I_1, 1:].flatten() / 100\n",
    "yields2 = data[I_2, 1:].flatten() / 100\n",
    "\n",
    "# Pchip Interpolation\n",
    "f1 = PchipInterpolator(Mat, yields1)\n",
    "Y_Int_1 = f1(MatInt)\n",
    "\n",
    "f2 = PchipInterpolator(Mat, yields2)\n",
    "Y_Int_2 = f2(MatInt)\n",
    "\n",
    "# Compute Zeros\n",
    "# Z = exp(-y * t) ? Guide uses continuous or semi-annual?\n",
    "# Usually Fama-Bliss yields are continuously compounded or annual?\n",
    "# Guide Q3 uses Z = exp(-(yield1+yshift)*maturity) which implies continuous.\n",
    "# Let's assume continuous for now. \n",
    "# Or semi-annual BEY? \n",
    "# \"compute relevant yields at semi-annual frequencies\" implies maybe BEY.\n",
    "# But usually Z = 1 / (1+y/2)^(2t)\n",
    "# Let's stick to BEY because we price LIF (coupon bond).\n",
    "\n",
    "Z1 = 1 / (1 + Y_Int_1 / 2)**(2 * MatInt)\n",
    "Z2 = 1 / (1 + Y_Int_2 / 2)**(2 * MatInt)\n",
    "\n",
    "# Plot Term Structure Change\n",
    "fig0 = go.Figure()\n",
    "fig0.add_trace(go.Scatter(x=MatInt, y=Y_Int_1, mode='lines', name='March 31, 2009'))\n",
    "fig0.add_trace(go.Scatter(x=MatInt, y=Y_Int_2, mode='lines', name='April 30, 2009'))\n",
    "fig0.update_layout(title='Change in Term Structure', xaxis_title='Maturity', yaxis_title='Yield')\n",
    "\n",
    "# LIF Valuation\n",
    "freq = 2\n",
    "coup_fix = 10\n",
    "TT = 5\n",
    "\n",
    "# Check dimensions. MatInt has length 10 (0.5 to 5.0). Matches TT*freq.\n",
    "\n",
    "CF_fixed = (coup_fix / freq) * ones(TT * freq)\n",
    "CF_fixed[-1] += 100\n",
    "\n",
    "P_Fixed1 = Z1 @ CF_fixed\n",
    "P_Fixed2 = Z2 @ CF_fixed\n",
    "\n",
    "P_Float = 100\n",
    "P_zero1 = 100 * Z1[-1]\n",
    "P_zero2 = 100 * Z2[-1]\n",
    "\n",
    "P_LIF1 = P_Fixed1 - 2 * P_Float + 2 * P_zero1\n",
    "P_LIF2 = P_Fixed2 - 2 * P_Float + 2 * P_zero2\n",
    "\n",
    "print(f'P_LIF March: {P_LIF1:.4f}')\n",
    "print(f'P_LIF April: {P_LIF2:.4f}')\n",
    "\n",
    "# Duration/Convexity Returns\n",
    "D_LIF_HW2 = 11.29 # From guide\n",
    "C_LIF_HW2 = 58.45 # From guide\n",
    "\n",
    "dr = mean(Y_Int_2 - Y_Int_1)\n",
    "\n",
    "dPP_D = -D_LIF_HW2 * dr\n",
    "dPP_D_C = -D_LIF_HW2 * dr + 0.5 * C_LIF_HW2 * (dr**2)\n",
    "\n",
    "actual_ret = (P_LIF2 - P_LIF1) / P_LIF1\n",
    "\n",
    "print(f'Dur-based Return: {dPP_D*100:.4f}%')\n",
    "print(f'Dur/Conv-based Return: {dPP_D_C*100:.4f}%')\n",
    "print(f'Actual Return: {actual_ret*100:.4f}%')\n",
    "\n",
    "# PCA\n",
    "loc = data[:, 0] <= date1\n",
    "YY = data[loc, 1:] / 100 # Historical yields up to date1\n",
    "T_obs = YY.shape[0]\n",
    "\n",
    "# Interpolate historical yields to MatInt\n",
    "yields_hist = zeros((T_obs, len(MatInt)))\n",
    "for ii in range(T_obs):\n",
    "    f = PchipInterpolator(Mat, YY[ii, :])\n",
    "    yields_hist[ii, :] = f(MatInt)\n",
    "    \n",
    "dy = diff(yields_hist, axis=0)\n",
    "SIGMA = np.cov(dy, rowvar=False, ddof=1)\n",
    "\n",
    "# Eigen decomposition\n",
    "vals, vecs = eigh(SIGMA)\n",
    "# eigh returns sorted ascending. We want descending.\n",
    "vals = vals[::-1]\n",
    "vecs = vecs[:, ::-1]\n",
    "\n",
    "E = diag(vals)\n",
    "\n",
    "# Compute factors (Level and Slope)\n",
    "# Factor 1 (Level)\n",
    "z1 = dy @ vecs[:, 0]\n",
    "Level = cumsum(z1) # Start from 0 or initial level? Guide just says cumsum(z1).\n",
    "\n",
    "# Regression to get Betas\n",
    "# dy_t = Beta * z1_t + error\n",
    "# We include intercept? Guide: \"z11=np.vstack((ones(T-1),z1)).T\"\n",
    "# Wait, T-1 because dy has length T-1.\n",
    "\n",
    "z11 = np.vstack((ones(len(z1)), z1)).T\n",
    "# b1 = inv(z11.T @ z11) @ z11.T @ dy\n",
    "# This regresses ALL yield changes on the first factor.\n",
    "# b1 shape: (2, N_maturities). Row 0 is alpha, Row 1 is beta.\n",
    "b1 = inv(z11.T @ z11) @ z11.T @ dy\n",
    "\n",
    "e1 = dy - z11 @ b1\n",
    "\n",
    "# Factor 2 (Slope)\n",
    "# Use residuals e1\n",
    "# Guide: \"z2=e1@V[:,-2]\" -> using 2nd eigenvector?\n",
    "# Note: eigh returns ascending. -1 is largest (Level). -2 is second largest (Slope).\n",
    "# Since I reversed vecs, index 0 is Level, index 1 is Slope.\n",
    "# But guide projects residuals e1 on eigenvector?\n",
    "# \"z2=e1@V[:,-2]\"\n",
    "# Usually factors are projected from data dy.\n",
    "# But guide does recursive extraction.\n",
    "# z2 = e1 @ vecs[:, 1]\n",
    "\n",
    "z2 = e1 @ vecs[:, 1]\n",
    "Slope = cumsum(z2)\n",
    "\n",
    "z22 = np.vstack((ones(len(z1)), z1, z2)).T\n",
    "b2 = inv(z22.T @ z22) @ z22.T @ dy\n",
    "\n",
    "# Betas\n",
    "# Row 0: Intercept\n",
    "# Row 1: Beta to Factor 1\n",
    "# Row 2: Beta to Factor 2\n",
    "\n",
    "betas = b2[1:3, :] # Shape (2, N_maturities)\n",
    "\n",
    "fig1 = go.Figure()\n",
    "fig1.add_trace(go.Scatter(x=MatInt, y=betas[0, :], mode='lines', name='Level Beta'))\n",
    "fig1.add_trace(go.Scatter(x=MatInt, y=betas[1, :], mode='lines', name='Slope Beta'))\n",
    "fig1.update_layout(title='Factor Betas', xaxis_title='Maturity', yaxis_title='Beta')\n",
    "\n",
    "# Factor Durations of LIF\n",
    "# D_LIF_Factor = Sum(w_i * D_i_Factor)\n",
    "# D_i_Factor = Beta_i (sensitivity of yield i to factor) * Duration_i?\n",
    "# Or D_P/dF = D_P/dy * dy/dF = -D_mod * Beta?\n",
    "# Yes, Factor Duration = Sum(Weight * (-D_i * Beta_i)) / Price?\n",
    "# Usually Factor Duration is defined as sensitivity to factor.\n",
    "# Price P = Sum(CF_i * Z_i(y_i)).\n",
    "# dP/dF = Sum( dP/dy_i * dy_i/dF )\n",
    "#       = Sum( (-D_i * P_i) * Beta_i ) where P_i is value of CF_i?\n",
    "# Actually dP/dy_i for a zero coupon bond is -D_i * P_i (modified duration).\n",
    "# For LIF, it's a portfolio.\n",
    "# Weights `stripweights` computed earlier for Fixed part.\n",
    "# Let's compute component durations.\n",
    "\n",
    "# MatInt corresponds to 0.5, 1.0 ... 5.0.\n",
    "# Betas correspond to these maturities.\n",
    "\n",
    "# Fixed Leg\n",
    "# Weights w_i = CF_i * Z_i / P_Fixed\n",
    "w_fixed = (CF_fixed * Z1) / P_Fixed1\n",
    "# Duration to Level = Sum(w_i * D_i * Beta_i_Level)\n",
    "# D_i = MatInt[i] / (1+y_i/2) (Modified Duration) or just MatInt[i] (Macaulay)?\n",
    "# dy is change in yield. P changes by -ModDur * dy.\n",
    "# If dy is BEY, ModDur = MacDur / (1+y/2).\n",
    "# Guide Q3 (PSET 2) used MacDur logic: \"D_Fixed = stripweights@maturity\".\n",
    "# And used \"dP = -D * P * dr\".\n",
    "# This implies D is Modified Duration if dr is change in yield.\n",
    "# But calculated as Sum(w*T). That's Macaulay.\n",
    "# If y is small, approx same.\n",
    "# But usually strictly ModDur = MacDur / (1+y/2).\n",
    "# Let's use Macaulay as per guide or assume continuous?\n",
    "# Guide PCA code computes \"betas\" which are sensitivities of yield to factor.\n",
    "# dy = Beta * dF.\n",
    "# dP/P = -D * dy = -D * Beta * dF.\n",
    "# So Factor Duration = D * Beta.\n",
    "\n",
    "# Let's assume D is just maturity (Macaulay) roughly or calculate ModDur properly.\n",
    "# Given PSET 2 guide: \"D_Fixed = stripweights@maturity\", it calculates Macaulay Duration.\n",
    "# And uses it in VaR as \"dP = -D * P * dr\".\n",
    "# This is an approximation (Dy ~ Dm).\n",
    "# I will stick to D = Maturity for consistency with guide.\n",
    "\n",
    "D_vec = MatInt\n",
    "\n",
    "# Fixed Leg Factor Durations\n",
    "Beta_L = betas[0, :]\n",
    "Beta_S = betas[1, :]\n",
    "\n",
    "D_Fixed_L = w_fixed @ (D_vec * Beta_L)\n",
    "D_Fixed_S = w_fixed @ (D_vec * Beta_S)\n",
    "\n",
    "# Floating Leg (Par)\n",
    "# Duration ~ 0.5. Maturity 0.5.\n",
    "# Beta at 0.5 (index 0).\n",
    "D_Float_L = 0.5 * Beta_L[0]\n",
    "D_Float_S = 0.5 * Beta_S[0]\n",
    "\n",
    "# Zero Leg (5 year)\n",
    "# Maturity 5.0 (index 9).\n",
    "D_Zero_L = 5.0 * Beta_L[-1]\n",
    "D_Zero_S = 5.0 * Beta_S[-1]\n",
    "\n",
    "# LIF Factor Durations\n",
    "# D_LIF = (D_Fix * P_Fix - 2 * D_Flt * P_Flt + 2 * D_Z * P_Z) / P_LIF\n",
    "\n",
    "D_LIF_L = (D_Fixed_L * P_Fixed1 - 2 * D_Float_L * P_Float + 2 * D_Zero_L * P_zero1) / P_LIF1\n",
    "D_LIF_S = (D_Fixed_S * P_Fixed1 - 2 * D_Float_S * P_Float + 2 * D_Zero_S * P_zero1) / P_LIF1\n",
    "\n",
    "print(f'Level Duration LIF: {D_LIF_L:.2f}')\n",
    "print(f'Slope Duration LIF: {D_LIF_S:.2f}')\n",
    "\n",
    "# Attribution\n",
    "# Change in Factors between March and April\n",
    "# We need Level and Slope values at date2.\n",
    "# We only computed Level/Slope for history up to date1.\n",
    "# We need change in factors for the specific period date1 to date2.\n",
    "# dy_new = Y_Int_2 - Y_Int_1\n",
    "# dF = Inverse(Beta) * dy_new? \n",
    "# Or project dy_new onto eigenvectors?\n",
    "# dy_new ~ Beta_L * dL + Beta_S * dS\n",
    "# We can estimate dL, dS by regressing dy_new on Betas.\n",
    "\n",
    "X_beta = betas.T\n",
    "dF = inv(X_beta.T @ X_beta) @ X_beta.T @ (Y_Int_2 - Y_Int_1)\n",
    "dLevel = dF[0]\n",
    "dSlope = dF[1]\n",
    "\n",
    "# Change in Price due to factors\n",
    "# dP/P = -D_L * dL - D_S * dS\n",
    "\n",
    "Diff_Level = -D_LIF_L * dLevel\n",
    "Diff_Slope = -D_LIF_S * dSlope\n",
    "\n",
    "dPP_Factors = Diff_Level + Diff_Slope\n",
    "\n",
    "print(f'Factor-based Return: {dPP_Factors*100:.4f}%')\n",
    "\n",
    "# =================== PART 2: Predictability ===========================\n",
    "print(\"Part 2: Predictability\")\n",
    "\n",
    "try:\n",
    "    # Sheet Annual\n",
    "    dataB = pd.read_excel(file_path, sheet_name='Annual', header=None, skiprows=5)\n",
    "    # Check shape\n",
    "    # DataB = DataB[:,0:31]\n",
    "    dataB = dataB.iloc[:, 0:31].values\n",
    "    \n",
    "    DateB = np.round(dataB[:, 0] / 100)\n",
    "    Yields = dataB[:, 1:6] # Indices 1 to 5\n",
    "    Fwd = dataB[:, 17:21]  # Indices 17 to 20\n",
    "    RetB = dataB[:, 25:29] # Indices 25 to 28\n",
    "    AveRetB = dataB[:, 29]\n",
    "    CP = dataB[:, 30]\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error reading Annual data: {e}\")\n",
    "    return\n",
    "\n",
    "# Regressions\n",
    "# Loop jpred 0 to 3 (n=2,3,4,5 bonds)\n",
    "# RetB column j corresponds to excess return of bond n=j+2\n",
    "\n",
    "TABLE_FB = []\n",
    "TABLE_CP = []\n",
    "\n",
    "def regression_stats(Y, X):\n",
    "    # OLS\n",
    "    # B = inv(X'X) X'Y\n",
    "    try:\n",
    "        model = sm.OLS(Y, X).fit()\n",
    "        return model.params, model.tvalues, model.rsquared\n",
    "    except:\n",
    "        return [0]*X.shape[1], [0]*X.shape[1], 0\n",
    "\n",
    "for jpred in range(4):\n",
    "    # YY: Excess return at t+1. \n",
    "    # Alignment: RetB row i is return from t to t+1? Or realized return at t?\n",
    "    # Usually data aligned such that row t has Return(t+1) and Predictor(t).\n",
    "    # Or row t has Return(t) and Predictor(t-1).\n",
    "    # Let's check \"RetB[1:,jpred]\".\n",
    "    # Guide code: \"YY=RetB[1:,jpred]\". \"XX0=ones...\". \"XX1=??\".\n",
    "    # This implies we regress RetB[1:] on Predictor[0:-1].\n",
    "    \n",
    "    YY = RetB[1:, jpred]\n",
    "    N = len(YY)\n",
    "    \n",
    "    # Fama-Bliss Predictor: Forward Spread -> f(n-1 -> n) - y(1) ?\n",
    "    # Or just Forward - Spot?\n",
    "    # Fama-Bliss (1987): Forward rate f_{t}^{(n, n-1)} - Spot rate y_{t}^{(1)} predicts excess return.\n",
    "    # Fwd columns: Indices 17-20. \n",
    "    # Mat 2,3,4,5. Fwd spreads usually provided or calculated?\n",
    "    # Fwd columns likely are the forward rates.\n",
    "    # Yields column 0 is 1-year yield (since columns 1-5 are 1y to 5y).\n",
    "    # So Spot 1y is Yields[:, 0].\n",
    "    # For bond n (jpred+2), we use Forward rate for n vs n-1?\n",
    "    # Fwd column jpred?\n",
    "    # Let's assume Fwd columns correspond to forwards for 2,3,4,5.\n",
    "    \n",
    "    # Predictor X: Fwd[0:-1, jpred] - Yields[0:-1, 0]\n",
    "    # Spread = Forward(n) - Yield(1)\n",
    "    \n",
    "    Spread = Fwd[0:-1, jpred] - Yields[0:-1, 0]\n",
    "    \n",
    "    X_FB = sm.add_constant(Spread)\n",
    "    \n",
    "    params, tvals, r2 = regression_stats(YY, X_FB)\n",
    "    TABLE_FB.append([jpred+2, params[0], params[1], tvals[0], tvals[1], r2])\n",
    "    \n",
    "    # Cochrane-Piazzesi\n",
    "    # Predictor: CP Factor\n",
    "    # CP[0:-1]\n",
    "    \n",
    "    X_CP = sm.add_constant(CP[0:-1])\n",
    "    params_cp, tvals_cp, r2_cp = regression_stats(YY, X_CP)\n",
    "    TABLE_CP.append([jpred+2, params_cp[0], params_cp[1], tvals_cp[0], tvals_cp[1], r2_cp])\n",
    "    \n",
    "TABLE_FB = pd.DataFrame(TABLE_FB, columns=['n', 'alpha', 'beta', 't_alpha', 't_beta', 'R2'])\n",
    "TABLE_CP = pd.DataFrame(TABLE_CP, columns=['n', 'alpha', 'beta', 't_alpha', 't_beta', 'R2'])\n",
    "\n",
    "print(\"Fama-Bliss Results:\")\n",
    "print(TABLE_FB)\n",
    "print(\"Cochrane-Piazzesi Results:\")\n",
    "print(TABLE_CP)\n",
    "\n",
    "return {\n",
    "    \"P_LIF1\": P_LIF1,\n",
    "    \"P_LIF2\": P_LIF2,\n",
    "    \"D_LIF_L\": D_LIF_L,\n",
    "    \"D_LIF_S\": D_LIF_S,\n",
    "    \"TABLE_FB\": TABLE_FB,\n",
    "    \"TABLE_CP\": TABLE_CP,\n",
    "    \"fig0\": fig0,\n",
    "    \"fig1\": fig1\n",
    "}\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
